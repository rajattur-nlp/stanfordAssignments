{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNvti9n2E8TOBj5RUS0H1zV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rajattur-nlp/stanfordAssignments/blob/main/Assignment2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "1FC1dTwQDepG"
      },
      "outputs": [],
      "source": [
        "#!/usr/bin/env python\n",
        "\n",
        "# Save parameters every a few SGD iterations as fail-safe\n",
        "SAVE_PARAMS_EVERY = 5000\n",
        "\n",
        "import glob\n",
        "import os.path as op\n",
        "import pickle\n",
        "import random\n",
        "import json\n",
        "\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Helper functions"
      ],
      "metadata": {
        "id": "VaFc0kybFGLL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def dump(obj, path):\n",
        "    with open(path, 'w') as f:\n",
        "        json.dump(obj, f)\n",
        "\n",
        "\n",
        "def load(path):\n",
        "    with open(path) as f:\n",
        "        obj = json.load(f)\n",
        "\n",
        "    return obj\n",
        "\n",
        "\n",
        "def normalize_rows(x):\n",
        "    \"\"\" Row normalization function\n",
        "\n",
        "    Implement a function that normalizes each row of a matrix to have\n",
        "    unit length.\n",
        "    \"\"\"\n",
        "    N = x.shape[0]\n",
        "    x /= np.sqrt(np.sum(x ** 2, axis=1)).reshape((N, 1)) + 1e-30\n",
        "    return x\n",
        "\n",
        "\n",
        "def softmax(x):\n",
        "    \"\"\"Compute the softmax function for each row of the input x.\n",
        "    It is crucial that this function is optimized for speed because\n",
        "    it will be used frequently in later code.\n",
        "\n",
        "    Arguments:\n",
        "    x -- A D dimensional vector or N x D dimensional numpy matrix.\n",
        "    Return:\n",
        "    x -- You are allowed to modify x in-place\n",
        "    \"\"\"\n",
        "    orig_shape = x.shape\n",
        "\n",
        "    if len(x.shape) > 1:\n",
        "        # Matrix\n",
        "        tmp = np.max(x, axis=1)\n",
        "        x -= tmp.reshape((x.shape[0], 1))\n",
        "        x = np.exp(x)\n",
        "        tmp = np.sum(x, axis=1)\n",
        "        x /= tmp.reshape((x.shape[0], 1))\n",
        "    else:\n",
        "        # Vector\n",
        "        tmp = np.max(x)\n",
        "        x -= tmp\n",
        "        x = np.exp(x)\n",
        "        tmp = np.sum(x)\n",
        "        x /= tmp\n",
        "\n",
        "    assert x.shape == orig_shape\n",
        "    return x"
      ],
      "metadata": {
        "id": "tSAebI0kE-PF"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sigmoid(x):\n",
        "  \"\"\"\n",
        "  Compute the sigmoid function for the input here.\n",
        "  Arguments:\n",
        "  x -- A scalar or numpy array.\n",
        "  Return:\n",
        "  s -- sigmoid(x)\n",
        "  \"\"\"\n",
        "\n",
        "  ### START CODE HERE\n",
        "  s = 1 / (1 + np.exp(-x))\n",
        "  ### END CODE HERE\n",
        "\n",
        "  return s"
      ],
      "metadata": {
        "id": "mDpobPAKFNbd"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def naive_softmax_loss_and_gradient(center_word_vec,outside_word_idx,outside_vectors,dataset):\n",
        "  \"\"\" Naive Softmax loss & gradient function for word2vec models\n",
        "\n",
        "  Implement the naive softmax loss and gradients between a center word's\n",
        "  embedding and an outside word's embedding. This will be the building block\n",
        "  for our word2vec models.\n",
        "\n",
        "  Arguments:\n",
        "  center_word_vec -- numpy ndarray, center word's embedding\n",
        "                  (v_c in the pdf handout)\n",
        "  outside_word_idx -- integer, the index of the outside word\n",
        "                  (o of u_o in the pdf handout)\n",
        "  outside_vectors -- outside vectors (rows of matrix) for all words in vocab\n",
        "                    (U in the pdf handout)\n",
        "  dataset -- needed for negative sampling, unused here.\n",
        "\n",
        "  Return:\n",
        "  loss -- naive softmax loss\n",
        "  grad_center_vec -- the gradient with respect to the center word vector\n",
        "                   (dJ / dv_c in the pdf handout)\n",
        "  grad_outside_vecs -- the gradient with respect to all the outside word vectors\n",
        "                  (dJ / dU)\n",
        "\n",
        "   Note:\n",
        "   - we usually use column vector convention (i.e., vectors are in column form) for vectors in matrix U and V (in the handout)\n",
        "   but for ease of implementation/programming we usually use row vectors (representing vectors in row form).\n",
        "   - A softmax() function provided (utils/utils.py) which takes as input a vector/matrix of values and returns the softmax for each value in the vector, relative to the others.\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  ### Please use the provided softmax function (imported earlier in this file)\n",
        "  ### This numerically stable implementation helps you avoid issues pertaining\n",
        "  ### to integer overflow.\n",
        "\n",
        "  ### START CODE HERE\n",
        "  # Scores for all outside words given the center word: U.dot(v_c)\n",
        "  scores = np.dot(outside_vectors, center_word_vec)  # Shape: (vocab_size,)\n",
        "\n",
        "  # Softmax probabilities\n",
        "  probs = softmax(scores)  # Shape: (vocab_size,)\n",
        "\n",
        "  # Naive softmax loss: -log(P(u_o|v_c))\n",
        "  loss = -np.log(probs[outside_word_idx])\n",
        "\n",
        "  # Gradients\n",
        "  # dJ / dv_c = U^T.(y_hat - y), where y is one-hot and y_hat is softmax probabilities\n",
        "  # y_hat - y results in setting the correct outside word idx to (prob - 1) and the rest to their probs\n",
        "  y_hat_minus_y = probs\n",
        "  y_hat_minus_y[outside_word_idx] -= 1  # Subtract 1 at the index of the true outside word\n",
        "\n",
        "  grad_center_vec = np.dot(outside_vectors.T, y_hat_minus_y)  # Shape: (embedding_dim,)\n",
        "\n",
        "  # dJ / dU = (y_hat - y).v_c^T\n",
        "  grad_outside_vecs = np.outer(y_hat_minus_y, center_word_vec)  # Shape: (vocab_size, embedding_dim)\n",
        "\n",
        "  ### END CODE HERE\n",
        "\n",
        "  return loss, grad_center_vec, grad_outside_vecs"
      ],
      "metadata": {
        "id": "6yfAWT4OFZAB"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_negative_samples(outside_word_idx, dataset, K):\n",
        "  \"\"\" Samples K indexes which are not the outsideWordIdx \"\"\"\n",
        "\n",
        "  neg_sample_word_indices = [None] * K\n",
        "  for k in range(K):\n",
        "    newidx = dataset.sample_token_idx()\n",
        "    while newidx == outside_word_idx:\n",
        "      newidx = dataset.sample_token_idx()\n",
        "    neg_sample_word_indices[k] = newidx\n",
        "  return neg_sample_word_indices"
      ],
      "metadata": {
        "id": "QkF15hlJHZDy"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def neg_sampling_loss_and_gradient(center_word_vec,outside_word_idx,outside_vectors,dataset,K=10):\n",
        "  \"\"\" Negative sampling loss function for word2vec models\n",
        "\n",
        "   Arguments/Return Specifications: same as naive_softmax_loss_and_gradient\n",
        "   K is the number of negative samples to take.\n",
        "\n",
        "   \"\"\"\n",
        "\n",
        "  neg_sample_word_indices = get_negative_samples(outside_word_idx, dataset, K)\n",
        "  indices = [outside_word_idx] + neg_sample_word_indices\n",
        "\n",
        "  grad_center_vec = np.zeros(center_word_vec.shape)\n",
        "  grad_outside_vecs = np.zeros(outside_vectors.shape)\n",
        "\n",
        "  labels = np.array([1] + [-1 for k in range(K)])\n",
        "  vecs = outside_vectors[indices, :]\n",
        "\n",
        "  t = sigmoid(vecs.dot(center_word_vec) * labels)\n",
        "  loss = -np.sum(np.log(t))\n",
        "\n",
        "  delta = labels * (t - 1)\n",
        "  grad_center_vec = delta.reshape((1, K + 1)).dot(vecs).flatten()\n",
        "  grad_outside_vecs_temp = delta.reshape((K + 1, 1)).dot(center_word_vec.reshape(\n",
        "    (1, center_word_vec.shape[0])))\n",
        "  for k in range(K + 1):\n",
        "    grad_outside_vecs[indices[k]] += grad_outside_vecs_temp[k, :]\n",
        "\n",
        "  return loss, grad_center_vec, grad_outside_vecs"
      ],
      "metadata": {
        "id": "mev26i_8HaEm"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def skipgram(current_center_word, window_size, outside_words, word2ind, center_word_vectors, outside_vectors, dataset, word2vec_loss_and_gradient=neg_sampling_loss_and_gradient):\n",
        "  \"\"\" Skip-gram model in word2vec\n",
        "\n",
        "  Implement the skip-gram model in this function.\n",
        "\n",
        "  Arguments:\n",
        "  current_center_word -- a string of the current center word\n",
        "  window_size -- integer, context window size\n",
        "  outside_words -- list of no more than 2*window_size strings, the outside words\n",
        "  word2ind -- a dictionary that maps words to their indices in\n",
        "            the word vector list\n",
        "  center_word_vectors -- center word vectors (as rows) for all words in vocab\n",
        "                        (V in pdf handout)\n",
        "  outside_vectors -- outside word vectors (as rows) for all words in vocab\n",
        "                  (U in pdf handout)\n",
        "  word2vec_loss_and_gradient -- the loss and gradient function for\n",
        "                             a prediction vector given the outsideWordIdx\n",
        "                             word vectors, could be one of the two\n",
        "                             loss functions you implemented above (do not hardcode any of them).\n",
        "\n",
        "  Return:\n",
        "  loss -- the loss function value for the skip-gram model\n",
        "          (J in the pdf handout)\n",
        "  grad_center_vecs -- the gradient with respect to the center word vectors\n",
        "          (dJ / dV in the pdf handout)\n",
        "  grad_outside_vectors -- the gradient with respect to the outside word vectors\n",
        "                      (dJ / dU in the pdf handout)\n",
        "  \"\"\"\n",
        "\n",
        "  loss = 0.0\n",
        "  grad_center_vecs = np.zeros(center_word_vectors.shape)\n",
        "  grad_outside_vectors = np.zeros(outside_vectors.shape)\n",
        "\n",
        "  ### START CODE HERE\n",
        "  center_word_idx = word2ind[current_center_word]\n",
        "  center_word_vec = center_word_vectors[center_word_idx]\n",
        "\n",
        "  for outside_word in outside_words:\n",
        "      outside_word_idx = word2ind[outside_word]\n",
        "\n",
        "      # Calculate the loss and gradients for the current outside word\n",
        "      current_loss, grad_center_vec, grad_outside_vecs = word2vec_loss_and_gradient(center_word_vec, outside_word_idx, outside_vectors, dataset=None)\n",
        "\n",
        "      loss += current_loss\n",
        "      grad_center_vecs[center_word_idx] += grad_center_vec\n",
        "      grad_outside_vectors += grad_outside_vecs\n",
        "  ### END CODE HERE\n",
        "\n",
        "  return loss, grad_center_vecs, grad_outside_vectors"
      ],
      "metadata": {
        "id": "oGSw79KPHf8V"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def word2vec_sgd_wrapper(word2vec_model, word2ind, word_vectors, dataset, window_size, word2vec_loss_and_gradient=neg_sampling_loss_and_gradient):\n",
        "  batchsize = 50\n",
        "  loss = 0.0\n",
        "  grad = np.zeros(word_vectors.shape)\n",
        "  N = word_vectors.shape[0]\n",
        "  center_word_vectors = word_vectors[:int(N / 2), :]\n",
        "  outside_vectors = word_vectors[int(N / 2):, :]\n",
        "  for i in range(batchsize):\n",
        "    window_size_1 = random.randint(1, window_size)\n",
        "    center_word, context = dataset.get_random_context(window_size_1)\n",
        "\n",
        "    c, gin, gout = word2vec_model(center_word, window_size_1, context, word2ind, center_word_vectors,outside_vectors, dataset, word2vec_loss_and_gradient)\n",
        "    loss += c / batchsize\n",
        "    grad[:int(N / 2), :] += gin / batchsize\n",
        "    grad[int(N / 2):, :] += gout / batchsize\n",
        "\n",
        "  return loss, grad"
      ],
      "metadata": {
        "id": "z9VsrohlJZpP"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_saved_params():\n",
        "    \"\"\"\n",
        "    A helper function that loads previously saved parameters and resets\n",
        "    iteration start.\n",
        "    \"\"\"\n",
        "    st = 0\n",
        "    for f in glob.glob(\"saved_params_*.npy\"):\n",
        "        iter = int(op.splitext(op.basename(f))[0].split(\"_\")[2])\n",
        "        if (iter > st):\n",
        "            st = iter\n",
        "\n",
        "    if st > 0:\n",
        "        params_file = \"saved_params_%d.npy\" % st\n",
        "        state_file = \"saved_state_%d.pickle\" % st\n",
        "        params = np.load(params_file)\n",
        "        with open(state_file, \"rb\") as f:\n",
        "            state = pickle.load(f)\n",
        "        return st, params, state\n",
        "    else:\n",
        "        return st, None, None"
      ],
      "metadata": {
        "id": "ul7ksvKYJaSt"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_params(iter, params):\n",
        "    params_file = \"saved_params_%d.npy\" % iter\n",
        "    np.save(params_file, params)\n",
        "    with open(\"saved_state_%d.pickle\" % iter, \"wb\") as f:\n",
        "        pickle.dump(random.getstate(), f)"
      ],
      "metadata": {
        "id": "NRsD_EjIJhZT"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sgd(f, x0, step, iterations, postprocessing=None, use_saved=False,PRINT_EVERY=10):\n",
        "  \"\"\" Stochastic Gradient Descent\n",
        "\n",
        "  Implement the stochastic gradient descent method in this function.\n",
        "\n",
        "  Arguments:\n",
        "  f -- the function to optimize, it should take a single\n",
        "       argument and yield two outputs, a loss and the gradient\n",
        "       with respect to the arguments\n",
        "  x0 -- the initial point to start SGD from\n",
        "  step -- the step size for SGD\n",
        "  iterations -- total iterations to run SGD for\n",
        "  postprocessing -- postprocessing function for the parameters\n",
        "                    if necessary. In the case of word2vec we will need to\n",
        "                    normalize the word vectors to have unit length.\n",
        "  PRINT_EVERY -- specifies how many iterations to output loss\n",
        "\n",
        "  Return:\n",
        "  x -- the parameter value after SGD finishes\n",
        "  \"\"\"\n",
        "\n",
        "  # Anneal learning rate every several iterations\n",
        "  ANNEAL_EVERY = 20000\n",
        "\n",
        "  if use_saved:\n",
        "    start_iter, oldx, state = load_saved_params()\n",
        "    if start_iter > 0:\n",
        "      x0 = oldx\n",
        "      step *= 0.5 ** (start_iter / ANNEAL_EVERY)\n",
        "\n",
        "    if state:\n",
        "      random.setstate(state)\n",
        "  else:\n",
        "    start_iter = 0\n",
        "\n",
        "  x = x0\n",
        "\n",
        "  if not postprocessing:\n",
        "    postprocessing = lambda x: x\n",
        "\n",
        "  exploss = None\n",
        "\n",
        "  for iter in range(start_iter + 1, iterations + 1):\n",
        "    # You might want to print the progress every few iterations.\n",
        "\n",
        "    loss = None\n",
        "    ### START CODE HERE\n",
        "    # Compute loss and gradient\n",
        "    loss, gradient = f(x)\n",
        "\n",
        "    # Update parameters\n",
        "    x -= step * gradient\n",
        "    ### END CODE HERE\n",
        "\n",
        "    x = postprocessing(x)\n",
        "    if iter % PRINT_EVERY == 0:\n",
        "      if not exploss:\n",
        "        exploss = loss\n",
        "      else:\n",
        "        exploss = .95 * exploss + .05 * loss\n",
        "      print(\"iter %d: %f\" % (iter, exploss))\n",
        "\n",
        "    if iter % SAVE_PARAMS_EVERY == 0 and use_saved:\n",
        "      save_params(iter, x)\n",
        "\n",
        "    if iter % ANNEAL_EVERY == 0:\n",
        "      step *= 0.5\n",
        "  return x"
      ],
      "metadata": {
        "id": "UoqwY3pxJkLC"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PobcqLN1Jq6s"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}